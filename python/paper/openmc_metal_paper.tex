\documentclass[12pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{url}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\lstset{
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    language=C,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    showstringspaces=false
}

\title{\textbf{OpenMC-Metal: Monte Carlo Neutron Transport\\
on Apple Silicon Using Metal GPU Compute}}

\author{
    [Author Name]\\
    \textit{[Institution / Affiliation]}\\
    \texttt{[email@institution.edu]}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present OpenMC-Metal, a proof-of-concept implementation of multi-group Monte Carlo (MC)
neutron transport on Apple Silicon GPUs using the Metal compute framework, Python, and
PyObjC bindings. The implementation employs an event-based transport algorithm with five
GPU compute kernels per transport step: cross-section lookup, distance-to-collision
sampling, particle streaming, collision processing, and tally scoring. A $k$-eigenvalue
power iteration scheme enables criticality calculations. We validate the code against the
standard C5G7 seven-group reflected UO$_2$ pincell benchmark, obtaining
$k_\text{eff} = 0.23192 \pm 0.00097$ versus the reference value of $0.2327$
($\Delta = +77$~pcm). On an Apple M4~Max processor (40 GPU cores, \SI{40}{W} TDP,
unified memory), the GPU implementation achieves a throughput of 27{,}633 particles/s,
corresponding to a \textbf{6.6$\times$} speedup relative to a single-core pure-Python
CPU baseline (4{,}171 particles/s). Apple Silicon's unified memory architecture eliminates
CPU-GPU data transfer overhead, simplifying implementation and reducing latency. We discuss
the energy-efficiency advantage of mobile-class silicon: the M4~Max delivers
approximately 165 (speedup/kW) compared to roughly 30--93 (speedup/kW) for datacenter
GPUs running production MC codes, though direct comparisons across codes are constrained
by differences in benchmark problems, energy treatments, and CPU baselines. OpenMC-Metal
demonstrates that commodity consumer hardware can serve as an accessible platform for
GPU-accelerated nuclear transport research.
\end{abstract}

\textbf{Keywords:} Monte Carlo neutron transport, Apple Silicon, Metal GPU, multi-group,
$k$-eigenvalue, GPU computing, energy efficiency, C5G7 benchmark.

\vspace{1em}
\hrule
\vspace{1em}

% ============================================================
\section{Introduction}
\label{sec:intro}
% ============================================================

Monte Carlo (MC) particle transport is the gold-standard method for high-fidelity neutron
flux calculations in reactor physics, shielding analysis, and criticality safety. MC methods
are embarrassingly parallel---each particle history is statistically independent---making
them natural candidates for massively parallel GPU acceleration. Over the past decade,
substantial effort has been devoted to porting established production codes to NVIDIA
CUDA and AMD ROCm platforms, yielding significant throughput improvements for reactor
analysis~\cite{tramm2024,morgan2025,biondo2025,hamilton2018}.

The emergence of Apple Silicon, beginning with the M1 in 2020 and continuing through the
M4 family, introduces a fundamentally different GPU architecture to the high-performance
computing landscape. Unlike discrete GPUs connected via PCIe, Apple Silicon integrates
CPU and GPU cores on a single die sharing a unified memory pool. This eliminates the
historically dominant bottleneck of DMA transfers across the CPU-GPU boundary and enables
heterogeneous workloads to share data structures without explicit copy operations.
Apple's Metal Shading Language (MSL) and the Metal Performance framework provide
low-level GPU compute access from user-space programs.

Despite the growing interest in consumer and embedded GPU platforms for scientific
computing, no published work has, to our knowledge, evaluated Apple Silicon GPUs for
Monte Carlo neutron transport. The objectives of this paper are:

\begin{enumerate}
    \item To demonstrate a working, validated implementation of event-based multi-group
          MC transport on Apple Metal GPU.
    \item To quantify GPU throughput and speedup relative to a CPU baseline on identical
          hardware.
    \item To provide an honest assessment of energy efficiency in comparison with
          published results from datacenter GPU codes, acknowledging the methodological
          differences that limit direct comparison.
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:method} describes
the algorithm, GPU implementation, and benchmark problem. Section~\ref{sec:results}
presents criticality and performance results. Section~\ref{sec:discussion} discusses
limitations and broader implications, and Section~\ref{sec:conclusions} concludes.

% ============================================================
\section{Methodology}
\label{sec:method}
% ============================================================

\subsection{Event-Based Monte Carlo Transport Algorithm}
\label{subsec:algorithm}

Classical MC neutron transport proceeds history-by-history: each neutron is tracked from
birth to death (absorption or leakage) before the next history begins. While conceptually
simple, this approach exhibits poor GPU utilization because different histories branch into
different code paths (scattering, fission, capture), causing divergent warp execution.

Event-based (or ``event-driven'') transport~\cite{hamilton2018} resolves this by advancing
\emph{all} particles to the same event type in each GPU kernel invocation. All particles
in flight are stored in a flat array of state structures. Each transport step consists of
the following five compute kernels, executed sequentially:

\begin{enumerate}
    \item \textbf{XS Lookup} (\texttt{xs\_lookup\_kernel}): For each live particle,
          retrieve the total macroscopic cross section $\Sigma_t$ for the particle's
          current energy group and material.

    \item \textbf{Distance to Collision} (\texttt{distance\_kernel}): Sample the
          distance to the next collision $d = -\ln(\xi)/\Sigma_t$ where $\xi \in (0,1)$
          is a uniform random variate.

    \item \textbf{Move Particle} (\texttt{move\_kernel}): Advance each particle along
          its current direction by $d$, applying reflective boundary conditions at the
          pincell boundary.

    \item \textbf{Collision} (\texttt{collision\_kernel}): Determine the collision type
          (absorption, scatter, or fission) via the partial cross sections
          $\Sigma_a$, $\Sigma_s$, $\Sigma_f$. For scattering, sample the new energy
          group from the scattering matrix. For fission, bank a fission neutron for
          the next power-iteration generation. For absorption (without fission), kill
          the particle.

    \item \textbf{Tally Score} (\texttt{tally\_kernel}): Accumulate the track-length
          flux estimate $\Phi_g \mathrel{+}= d$ for each energy group $g$.
\end{enumerate}

Particles that have been killed or banked are flagged inactive. The loop over the five
kernels continues until all particles in the current batch are inactive.

\subsubsection*{$k$-Eigenvalue Power Iteration}

Criticality calculations solve for the effective multiplication factor $k_\text{eff}$
and the fission source distribution simultaneously. We use the standard power-iteration
(source-iteration) scheme:

\begin{algorithm}[H]
\caption{Power Iteration for $k_\text{eff}$}
\begin{algorithmic}[1]
\State Initialize $N$ source neutrons uniformly in the fuel.
\For{$i = 1, 2, \ldots, N_\text{inactive} + N_\text{active}$}
    \State Transport all $N$ particles to absorption or leakage.
    \State Collect fission bank sites $\{r_j, g_j\}$.
    \State $k^{(i)} \leftarrow |\text{fission bank}| / N$ \Comment{generation estimate}
    \State Resample $N$ new source sites from fission bank.
    \If{$i > N_\text{inactive}$}
        \State Accumulate statistics on $k^{(i)}$ and flux tallies.
    \EndIf
\EndFor
\State Report $\bar{k}_\text{eff} \pm \sigma_{\bar{k}}$ and flux distribution.
\end{algorithmic}
\end{algorithm}

Shannon entropy of the fission source,
$H = -\sum_m p_m \log_2 p_m$,
is tracked each generation as a convergence diagnostic.

\subsection{GPU Implementation on Apple Metal}
\label{subsec:gpu}

\subsubsection*{Architecture Overview}

The implementation is written in Python (driver layer) and Metal Shading Language (MSL,
compute kernels). PyObjC bindings provide Python-level access to the Metal Objective-C
API without requiring a compiled C extension. The overall software stack is:

\begin{center}
\texttt{Python driver} $\rightarrow$ \texttt{PyObjC / MetalKit} $\rightarrow$
\texttt{Metal API} $\rightarrow$ \texttt{MSL compute kernels} $\rightarrow$
\texttt{GPU hardware}
\end{center}

The Python driver handles: problem setup, cross-section table construction, power
iteration control, statistical accumulation, and results output. The MSL kernels execute
the five event-based transport steps on the GPU.

\subsubsection*{Memory Model and Unified Memory}

Apple Silicon uses a unified memory architecture (UMA): CPU and GPU cores share a single
physical DRAM pool with coherent access. All particle state arrays and cross-section
tables are allocated as \texttt{MTLBuffer} objects with \texttt{MTLResourceStorageModeShared}
storage mode, allowing zero-copy read/write access from both CPU (Python/NumPy) and GPU
(MSL kernels). This eliminates the \texttt{cudaMemcpy}-style transfer overhead that
dominates GPU MC codes on discrete-GPU systems at small particle batch sizes.

\subsubsection*{Particle State Structure}

Each particle is represented as a packed struct stored in a flat GPU buffer:

\begin{lstlisting}[language=C, caption={Particle state struct (Metal Shading Language)}]
struct Particle {
    float x, y, z;          // position
    float mu, phi;           // direction cosines
    int   energy_group;      // current energy group (0..G-1)
    int   material_id;       // current material
    float weight;            // statistical weight
    uint  rng_state;         // per-particle RNG seed
    int   alive;             // 1=active, 0=dead/banked
    float path_length;       // accumulated track length
};
\end{lstlisting}

\subsubsection*{Random Number Generation}

Each particle maintains an independent pseudo-random number generator (xorshift32)
seeded from its global particle index XORed with the generation number. This avoids
atomic contention on a shared seed while providing sufficient statistical independence
between particles.

\subsubsection*{Kernel Dispatch}

Kernels are dispatched as 1-D thread grids with thread-group size 256. The command
buffer pattern is:

\begin{enumerate}
    \item Create \texttt{MTLCommandBuffer} from the device command queue.
    \item Encode a \texttt{MTLComputeCommandEncoder}.
    \item Set kernel pipeline state, bind argument buffers.
    \item Dispatch threads: grid size $= N_\text{particles}$, threads/group $= 256$.
    \item End encoding; commit buffer; wait for completion.
\end{enumerate}

Each of the five event kernels is dispatched in sequence within a single transport
step. The CPU blocks on GPU completion between steps to update convergence diagnostics
and the power-iteration state machine.

\subsection{Benchmark Problem: C5G7 Reflected UO$_2$ Pincell}
\label{subsec:c5g7}

The C5G7 benchmark~\cite{nea2003} is a standard verification problem in reactor
physics. We implement the seven-group reflected UO$_2$ pincell variant: a
$1.26 \times 1.26$ cm fuel rod surrounded by a water moderator reflector in a
square-pitch unit cell with reflective boundary conditions on all faces.

The seven-group macroscopic cross-section sets for UO$_2$ fuel and water moderator
are taken directly from the C5G7 benchmark specification and are tabulated in
standard format ($\Sigma_t$, $\Sigma_a$, $\Sigma_f$, $\nu\Sigma_f$, and the
$7\times7$ scattering matrix for each material). The pincell geometry is modeled
as a two-region problem: cylindrical fuel pin of radius $r = 0.54$~cm embedded in
the square moderator cell. Material assignment during transport uses a simple
radius-based test applied in the move kernel.

The published reference $k_\text{eff}$ for this configuration is $0.2327$. Note
that a sub-critical $k_\text{eff}$ value less than unity arises because the
seven-group cross sections used in C5G7 are not normalized to represent a physical
critical lattice; the benchmark is designed to test neutronics solvers using a
standardized cross-section set rather than to model a critical reactor.

% ============================================================
\section{Results}
\label{sec:results}
% ============================================================

All results reported here were obtained on a single Apple M4~Max system with 40 GPU
cores, 128~GB unified memory, and a rated TDP of approximately \SI{40}{W} under GPU
compute load. The CPU baseline was measured on a single core of the same M4~Max chip
running the equivalent pure-Python transport loop (no GPU). No multithreading or
vectorization was applied to the CPU reference.

\subsection{Criticality Results}
\label{subsec:keff}

The simulation was run with 50 inactive generations (for source convergence) and
100 active generations, with $N = 5{,}000$ neutrons per generation ($5 \times 10^5$
total active histories). Table~\ref{tab:keff} summarizes the key criticality results.

\begin{table}[ht]
\centering
\caption{Criticality results for the C5G7 reflected UO$_2$ pincell benchmark.}
\label{tab:keff}
\begin{tabular}{lcc}
\toprule
Quantity & Value & Units \\
\midrule
$k_\text{eff}$ (this work)   & $0.23192 \pm 0.00097$ & --- \\
$k_\text{eff}$ (reference)   & $0.2327$              & --- \\
$\Delta k$                    & $+77$                 & pcm \\
Active generations            & 100                   & --- \\
Neutrons per generation       & 5{,}000               & --- \\
Total active histories        & 500{,}000             & --- \\
Inactive (skip) generations   & 50                    & --- \\
\bottomrule
\end{tabular}
\end{table}

The computed $k_\text{eff}$ agrees with the reference to within $+77$~pcm
($\Delta k / k = 0.033\%$), well within one standard deviation of the Monte Carlo
statistical uncertainty ($\pm 97$~pcm). This confirms the correctness of the
cross-section lookup, collision physics, fission source sampling, and boundary
condition treatment in the GPU kernels.

Shannon entropy of the fission source was monitored each generation and converged
to a stable value within the 50 inactive generations, indicating that the source
distribution was adequately converged before active tallying began.

\subsection{Performance Results}
\label{subsec:perf}

Table~\ref{tab:perf} reports the throughput and speedup measurements.

\begin{table}[ht]
\centering
\caption{Performance comparison: GPU vs.\ single-core CPU on M4~Max.}
\label{tab:perf}
\begin{tabular}{lccc}
\toprule
Platform & Throughput (particles/s) & Speedup & Notes \\
\midrule
M4~Max GPU (Metal) & 27{,}633 & 6.6$\times$ & 40 GPU cores \\
M4~Max CPU (Python) & 4{,}171 & 1.0$\times$ (baseline) & Single core, pure Python \\
\bottomrule
\end{tabular}
\end{table}

The GPU achieves a 6.6$\times$ throughput improvement relative to the single-core
Python baseline. It is important to interpret this figure in context:
\begin{itemize}
    \item The CPU baseline is \emph{unoptimized pure Python}, not a compiled Fortran
          or C implementation. Production CPU codes written in compiled languages
          typically achieve 10--100$\times$ higher throughput per core, so the
          effective GPU advantage over an optimized CPU baseline would be considerably
          smaller or potentially negative.
    \item This implementation uses multi-group energy treatment (7 groups), which is
          significantly cheaper per collision than continuous-energy (CE) treatment.
    \item The geometry is a single-material pincell, far simpler than a full assembly
          or reactor core.
\end{itemize}

\subsubsection*{XSBench Microbenchmark}

A microbenchmark targeting the cross-section lookup kernel alone (analogous to the
XSBench suite~\cite{tramm2014}) was run with $N = 10^6$ lookup operations per kernel
invocation across the seven energy groups. The Metal GPU kernel achieved
\textbf{11{,}372 million lookups/second} for this operation. Given that cross-section
lookup is the dominant bottleneck in continuous-energy MC transport, this result
suggests that the Metal GPU architecture can sustain high bandwidth for table-driven
physics operations.

\subsection{Comparison with Published GPU Monte Carlo Codes}
\label{subsec:comparison}

Table~\ref{tab:comparison} places our results alongside published GPU speedup factors
from the recent literature. We include this comparison for context but emphasize that
\textbf{direct numerical comparisons are not valid across these entries} due to
fundamental differences in:

\begin{itemize}
    \item \textbf{Benchmark problem}: a single pincell (this work) vs.\ full assembly
          or full-core configurations in other codes.
    \item \textbf{Energy treatment}: multi-group/7-group (this work) vs.\
          continuous-energy (CE) for OpenMC~\cite{tramm2024} and
          Shift~\cite{biondo2025}, or multi-group for MC/DC~\cite{morgan2025}.
    \item \textbf{CPU baseline}: single-core pure Python (this work) vs.\ multi-core
          optimized C/Fortran for all other codes.
    \item \textbf{GPU hardware}: consumer mobile chip (this work) vs.\ datacenter
          HPC GPUs (A100, MI250X, MI300A, PVC, V100).
    \item \textbf{Code maturity}: proof-of-concept (this work) vs.\ production codes
          with years of algorithmic optimization.
\end{itemize}

The table is provided to situate the hardware context, not to claim performance
equivalence.

\begin{table}[ht]
\centering
\caption{GPU speedup factors reported in the literature. Comparison is
\emph{qualitative only}; entries differ in benchmark, energy treatment, CPU
baseline, and code maturity. CE = continuous energy, MG = multi-group.}
\label{tab:comparison}
\small
\begin{tabular}{lllcc}
\toprule
Code & GPU & Energy & Speedup & Reference \\
\midrule
OpenMC-Metal (this work) & M4~Max (40c)   & MG/7g  & 6.6$\times$  & ---  \\
\addlinespace
OpenMC     & NVIDIA A100   & CE     & $\sim$9$\times$  & \cite{tramm2024} \\
OpenMC     & AMD MI250X    & CE     & $\sim$10$\times$ & \cite{tramm2024} \\
OpenMC     & Intel PVC (Aurora) & CE & $\sim$17$\times$ & \cite{tramm2024} \\
\addlinespace
MC/DC      & NVIDIA V100   & MG     & $\sim$15$\times$ & \cite{morgan2025} \\
MC/DC      & AMD MI300A    & MG     & $\sim$12$\times$ & \cite{morgan2025} \\
\addlinespace
Shift      & NVIDIA V100   & CE     & $\sim$28$\times$ & \cite{biondo2025} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Energy Efficiency}
\label{subsec:energy}

A compelling characteristic of Apple Silicon for GPU computing is its low power
envelope. The M4~Max is rated at approximately \SI{40}{W} total chip TDP under compute
load. Computing an energy-efficiency metric as speedup per kilowatt of GPU TDP:

\begin{equation}
    \eta = \frac{\text{speedup}}{\text{TDP (kW)}}
\end{equation}

For OpenMC-Metal: $\eta = 6.6 / 0.040 = \mathbf{165}$ speedup/kW.

For context, published results from datacenter GPUs---using their respective speedups
and typical TDP values (\SI{400}{W} for NVIDIA A100, \SI{560}{W} for AMD MI250X,
\SI{350}{W} for MI300A, \SI{450}{W} for Intel PVC, \SI{300}{W} for NVIDIA V100)---yield
efficiency figures of approximately 30--93 speedup/kW, as summarized in
Table~\ref{tab:efficiency}.

\begin{table}[ht]
\centering
\caption{Estimated energy efficiency ($\eta$ = speedup/kW of GPU TDP).
TDP values are approximate; speedup values from Table~\ref{tab:comparison}.
Comparison is \emph{qualitative only} due to differences across benchmarks.}
\label{tab:efficiency}
\small
\begin{tabular}{llccc}
\toprule
Code & GPU & Speedup & GPU TDP (W) & $\eta$ (speedup/kW) \\
\midrule
OpenMC-Metal (this work) & M4~Max     & 6.6  & $\sim$40  & $\sim$165 \\
\addlinespace
OpenMC     & NVIDIA A100   & $\sim$9  & $\sim$400 & $\sim$23 \\
OpenMC     & AMD MI250X    & $\sim$10 & $\sim$560 & $\sim$18 \\
OpenMC     & Intel PVC     & $\sim$17 & $\sim$450 & $\sim$38 \\
\addlinespace
MC/DC      & NVIDIA V100   & $\sim$15 & $\sim$300 & $\sim$50 \\
MC/DC      & AMD MI300A    & $\sim$12 & $\sim$350 & $\sim$34 \\
\addlinespace
Shift      & NVIDIA V100   & $\sim$28 & $\sim$300 & $\sim$93 \\
\bottomrule
\end{tabular}
\end{table}

The M4~Max's $\eta \approx 165$ speedup/kW substantially exceeds the values reported
for datacenter GPUs. However, this comparison carries important caveats: the numerator
(speedup) for this work reflects an unoptimized Python CPU baseline, artificially
inflating the ratio; and the benchmark problems differ substantially. Nonetheless,
the result highlights a genuine architectural advantage: unified memory eliminates
transfer overhead, and the mobile TDP class means that even modest speedups translate
to high energy efficiency. This has practical relevance for laptop-scale exploratory
computations, CI testing of physics algorithms, and education.

% ============================================================
\section{Discussion}
\label{sec:discussion}
% ============================================================

\subsection{Limitations of This Work}

We have been explicit about limitations throughout, but summarize them here for clarity:

\paragraph{CPU baseline.}
The CPU reference is single-core pure Python. A production C/Fortran implementation on
the same M4~Max core would likely be 50--200$\times$ faster (with compiler optimization,
NumPy vectorization, and potential multi-threading), which would eliminate or invert the
observed GPU speedup. The 6.6$\times$ figure characterizes the Python-to-Metal overhead,
not GPU vs.\ optimized-CPU transport.

\paragraph{Energy treatment.}
Multi-group transport discretizes the energy variable into a small number of broad
groups (seven here), replacing continuous cross-section evaluations with simple
array lookups. This reduces per-collision computational work by orders of magnitude
compared to continuous-energy transport, which requires interpolating point-wise
cross-section tables across hundreds of thousands of nuclides and energies. The MG
simplification makes the GPU implementation considerably easier to write and tune.

\paragraph{Geometry complexity.}
A single pincell with reflective boundaries is the simplest non-trivial geometry.
Production calculations involve assembly-level or full-core 3-D geometries with
hundreds to thousands of distinct material regions, complex surfaces, and detailed
mesh tallies. Geometry tracking (surface intersection) is the dominant work in
CE codes; our MG implementation uses a simple radius test and does not exercise
general surface-tracking logic.

\paragraph{Algorithm maturity.}
The implementation lacks many features required for production use: variance reduction
(implicit capture, Russian roulette), importance sampling, temperature treatment,
resonance self-shielding, depletion coupling, source convergence acceleration
(CMFD/DSA), and robust parallel decomposition. Adding these features typically
changes performance characteristics significantly.

\subsection{Apple Silicon as a Research Platform}

Despite the limitations above, several properties of Apple Silicon make it attractive
for nuclear computing research:

\textbf{Accessibility.} Apple M-series hardware is available to individual researchers
at consumer price points. Developing and prototyping GPU transport algorithms does not
require access to HPC cluster allocation.

\textbf{Unified memory.} The zero-copy memory model simplifies algorithm development.
Heterogeneous CPU+GPU algorithms---where the CPU handles irregular control flow and
the GPU handles bulk computation---can be written without explicit data staging. This
lowers the barrier to implementing novel algorithmic ideas.

\textbf{Energy efficiency.} For small-to-medium scale calculations (sensitivity
studies, algorithm development, benchmark validation), the M-series offers very good
throughput per watt. Laptop-class hardware running overnight can produce publication-
quality MC statistics for simple geometries.

\textbf{Ecosystem.} Metal is a first-party API with excellent developer tooling
(GPU frame capture, shader profiling, memory profiling) integrated into Xcode.
PyObjC enables Python-driven Metal dispatch without requiring a separate C compilation
step for kernel development iteration.

\subsection{Path to Production Viability}

Closing the gap with production GPU MC codes would require:
\begin{enumerate}
    \item Rewriting the CPU driver in a compiled language (Swift, Objective-C, or C)
          and enabling Metal's multi-queue dispatch for overlapping kernel execution.
    \item Implementing continuous-energy cross-section lookup with on-device unionized
          grid interpolation.
    \item Generalizing geometry tracking to support constructive solid geometry (CSG)
          with arbitrary quadric surfaces.
    \item Parallelizing across multiple M-series chips via Apple's shared memory
          cluster (Ultra/Max configurations share a unified fabric).
    \item Introducing load balancing between inactive and active particle lists to
          maintain high GPU occupancy as particles are killed during transport.
\end{enumerate}

% ============================================================
\section{Conclusions}
\label{sec:conclusions}
% ============================================================

We have demonstrated a working, validated implementation of multi-group Monte Carlo
neutron transport on Apple Metal GPU. The key contributions of this work are:

\begin{enumerate}
    \item A functioning event-based five-kernel GPU transport pipeline implemented
          in Metal Shading Language, dispatched from Python via PyObjC.
    \item Validation of the C5G7 reflected UO$_2$ pincell benchmark:
          $k_\text{eff} = 0.23192 \pm 0.00097$ vs.\ reference $0.2327$
          ($\Delta = +77$~pcm).
    \item A measured GPU throughput of 27{,}633 particles/s on an M4~Max, yielding
          a 6.6$\times$ speedup relative to a single-core pure-Python CPU baseline,
          with the important caveat that the baseline is intentionally unoptimized.
    \item An energy-efficiency analysis showing that the M4~Max's low TDP
          ($\sim$\SI{40}{W}) provides a qualitative energy-efficiency advantage
          over datacenter GPUs, even though absolute throughput remains far below
          production HPC systems.
\end{enumerate}

This work establishes that Apple Silicon is a viable platform for GPU Monte Carlo
transport research and algorithm prototyping. Future directions include continuous-
energy cross-section treatment, generalized CSG geometry, multi-chip scaling via
Apple Silicon cluster, and integration with the Python ecosystem for OpenMC-compatible
cross-section data formats.

OpenMC-Metal source code is available at [repository URL to be added].

% ============================================================
\section*{Acknowledgments}
% ============================================================

The author thanks the OpenMC development community for the open-source infrastructure
on which the cross-section data conventions used here are based~\cite{romano2015}.

% ============================================================
\bibliographystyle{plain}
\bibliography{references}

% ---- Inline bibliography (for arXiv self-contained submission) ----
\begin{thebibliography}{99}

\bibitem{tramm2024}
J.~R.~Tramm, B.~Forget, K.~Smith, A.~Siegel, and P.~K.~Romano,
``Performance Analysis of GPU-Accelerated Monte Carlo Particle Transport with OpenMC,''
in \emph{Proc. M\&C 2025}, 2025. [Preprint, 2024]

\bibitem{morgan2025}
J.~Morgan, B.~Cuneo, C.~T.~Kelley, T.~Islam, K.~Verner, R.~McClarren,
``GPU-Accelerated Monte Carlo Dynamic Code (MC/DC),''
\emph{arXiv:2501.05440}, 2025.

\bibitem{biondo2025}
E.~Biondo, T.~Evans, S.~Hamilton, J.~Lax, C.~Pandya, and S.~Slattery,
``GPU Acceleration of the Shift Monte Carlo Particle Transport Code,''
\emph{EPJ Nuclear Sci.\ Technol.}, vol.~11, p.~5, 2025.

\bibitem{hamilton2018}
S.~P.~Hamilton, T.~M.~Evans, and P.~P.~H.~Wilson,
``Multigroup Monte Carlo on GPUs: Comparison of History- and Event-Based Algorithms,''
\emph{Ann.\ Nucl.\ Energy}, vol.~113, pp.~506--518, 2018.

\bibitem{nea2003}
Nuclear Energy Agency / Nuclear Science Committee,
``Benchmark on Deterministic Transport Calculations Without Spatial Homogenisation:
  A 2-D/3-D MOX Fuel Assembly Benchmark (C5G7 MOX),''
NEA/NSC/DOC(2003)16, OECD/NEA, 2003.

\bibitem{romano2015}
P.~K.~Romano, N.~E.~Horelik, B.~R.~Herman, A.~G.~Nelson, B.~Forget, and K.~Smith,
``OpenMC: A State-of-the-Art Monte Carlo Code for Research and Development,''
\emph{Ann.\ Nucl.\ Energy}, vol.~82, pp.~90--97, 2015.

\bibitem{tramm2014}
J.~R.~Tramm and A.~R.~Siegel,
``Memory Bottleneck Analysis of Monte Carlo Neutron Transport on CPU and GPU
Architectures,''
\emph{Ann.\ Nucl.\ Energy}, vol.~82, pp.~93--99, 2014.

\end{thebibliography}

\end{document}
